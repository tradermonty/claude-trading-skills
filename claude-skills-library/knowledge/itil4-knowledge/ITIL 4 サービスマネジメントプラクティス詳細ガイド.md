# ITIL 4 サービスマネジメントプラクティス詳細ガイド

# ITIL 4 サービスマネジメントプラクティス詳細ガイド

このドキュメントは、ITIL 4 が定義する 17 のサービスマネジメントプラクティスを深掘りし、社内ナレッジ共有と自己学習を目的として作成しています。各プラクティスを **目的・価値** → **主要活動** → **成果指標** → **ロール** → **イン／アウトプット** → **関連プラクティス** → **失敗要因** → **成熟度目安** → **ツール例** という 9 つの切り口で整理しました。

***

## 目次

1. Availability Management

2. Business Analysis

3. Capacity & Performance Management

4. Change Enablement

5. Incident Management

6. IT Asset Management

7. Monitoring & Event Management

8. Problem Management

9. Release Management

10. Service Catalog Management

11. Service Configuration Management

12. Service Continuity Management

13. Service Design

14. Service Desk

15. Service Level Management

16. Service Request Management

17. Service Validation & Testing

***

## 1. Availability Management

### 1.1 目的・価値

- 合意済みの可用性・復旧性 (MTBF, MTRS) を継続的に満たし、顧客の事業損失リスクを最小化する。

### 1.2 主要活動

1. **可用性要件の収集** (SLO, RPO/RTO)。

2. **可用性モデル設計** (冗長化パターン、SPOF 排除)。

3. **サービスイン後の監視とレポート** (稼働率, MTBF, MTTR)。

4. **レビューと改善提案** (キャパシティ, DR, コスト最適化)。

### 1.3 成果指標 (例)

- 稼働率 99.95% 以上

- 計画外停止時間 ≤ 30 分/四半期

- SPOF 解消率 100%

### 1.4 インプット／アウトプット

| インプット            | アウトプット                   |
| ---------------- | ------------------------ |
| ビジネス可用性要求, 技術アーキ | 可用性設計文書, SLA 更新, 改善バックログ |

### 1.5 ロールと責任

- **可用性マネージャ**: プラクティスオーナ。

- **サービスアーキテクト**: 冗長化設計実装。

- **サイトリライアビリティエンジニア (SRE)**: 可用性監視・改善。

### 1.6 関連プラクティス

- Capacity & Performance, Service Continuity, Monitoring & Event。

### 1.7 失敗しがちなポイント

- "稼働率＝可用性" と誤解しリカバリ時間を見落とす。

- SPOF 対策が CAPEX 増大で却下されフェーズアウト。

### 1.8 成熟度チェックポイント

| レベル      | 特徴                          |
| -------- | --------------------------- |
| Lv1: 初期  | 可用性目標が口頭レベル。                |
| Lv3: 標準化 | 可用性モデル標準、MTBF 自動計測。         |
| Lv5: 最適化 | SRE 原則に基づき SLO とエラーバジェット運用。 |

### 1.9 ツール例

- Pingdom・Datadog (稼働率監視)

- Chaos Engineering (Gremlin)

- AWS Well‑Architected Tool

***

## 2. Business Analysis

### 2.1 目的・価値

- ビジネス要件をサービス要件へ正確に変換し、価値を最大化。

### 2.2 主要活動

1. ステークホルダー分析とペルソナ定義。

2. 要求 & ストーリー管理 (Backlog)。

3. プロセスモデリング (BPMN)。

4. 価値ストリーム設計 & ROI 算出。

### 2.3 成果指標

- 要件変更率 < 10% (Sprint 中)。

- MVP リリース毎の NPS > 50。

### 2.4 インプット／アウトプット

| インプット         | アウトプット                        |
| ------------- | ----------------------------- |
| ビジネスケース, アイデア | 要件定義書, User Story Map, MVP 計画 |

### 2.5 ロール

- **ビジネスアナリスト**, **プロダクトオーナ**

### 2.6 関連プラクティス

- Service Design, Portfolio Management。

### 2.7 失敗しがちなポイント

- 要件を "機能" のみで捉え価値仮説が欠落。

### 2.8 成熟度

| レベル | 特徴               |
| --- | ---------------- |
| Lv2 | 単発プロジェクト毎の要求把握。  |
| Lv4 | 価値ストリームに要求管理が統合。 |

### 2.9 ツール例

- Jira, Miro, BPMN ツール (Camunda)。

***

## 3. Capacity & Performance Management

### 3.1 目的・価値

- 将来需要を予測し、性能・コストの最適バランスを実現。

### 3.2 主要活動

1. 需要予測 (ビジネス指標×シーズン変動)。

2. パフォーマンス・キャパシティ計画。

3. プロアクティブなスケールアウト。

4. キャパシティレビュー & 改善。

### 3.3 成果指標

- リソース不足によるパフォーマンス障害 0 件/月。

### 3.4 インプット／アウトプット

| インプット         | アウトプット      |
| ------------- | ----------- |
| 需要予測, 現行性能データ | 容量計画書, 予算要求 |

### 3.5 ロール

- **キャパシティマネージャ**

### 3.6 関連プラクティス

- Financial Management, Availability。

### 3.7 失敗しがちなポイント

- 過去トレンドのみで予測し新規プロジェクト需要を漏らす。

### 3.8 成熟度

| レベル | 特徴                           |
| --- | ---------------------------- |
| Lv3 | 月次キャパシティレビューと閾値ベースアラート       |
| Lv5 | AIOps による需要予測とリアルタイム自動スケーリング |

### 3.9 ツール例 ツール例

- Kubernetes HPA, AWS Compute Optimizer。

***

## 4. Change Enablement

### 4.1 目的・価値

- リスクを最小化しながら価値ある変更を迅速に本番環境へ反映し、ビジネスのアジリティを高める。

### 4.2 主要活動

1. 変更タイプ判定 (Standard, Normal, Emergency)。

2. Impact/Risk アセスメントと承認ワークフロー (CAB/ECAB)。

3. CI/CD パイプライン統合、自動テスト。

4. 変更カレンダー運用と事後レビュー。

### 4.3 成果指標

- 変更失敗率 < 5%

- 正常リリースまでのリードタイム ≤ 24 時間 (標準変更)

- 緊急変更比率 < 10%

### 4.4 インプット／アウトプット

| インプット            | アウトプット                                        |
| ---------------- | --------------------------------------------- |
| 変更要求 (RFC), 影響分析 | 承認済み変更計画, 更新 CMDB, Post‑Implementation Review |

### 4.5 ロール

- **変更マネージャ**, **CAB**, **デリバリーエンジニア**

### 4.6 関連プラクティス

- Release, Deployment, Service Validation, Risk Management。

### 4.7 失敗しがちなポイント

- CAB が形骸化しリードタイムを延ばす。

- 標準変更テンプレートが更新されず例外対応が乱発。

### 4.8 成熟度

| レベル | 特徴                            |
| --- | ----------------------------- |
| Lv2 | 手動承認フロー。                      |
| Lv4 | API 駆動の自動デプロイ、テストカバレッジ > 80%。 |

### 4.9 ツール例

- Jira Service Management, GitHub Actions, LaunchDarkly。

***

## 5. Incident Management

### 5.1 目的・価値

- サービス中断によるビジネス影響を最小化し、迅速な復旧で顧客体験を守る。

### 5.2 主要活動

1. アラートとインシデント自動生成 (監視統合)。

2. 優先度付与 (影響×緊急度)。

3. エスカレーションとコミュニケーション。

4. 復旧 (修正 / 回避策) と Post‑Incident Review (PIR)。

### 5.3 成果指標

- 平均復旧時間 (MTTR) ≤ 30 分 (P1)。

- インシデント顧客満足度 (CSAT) ≥ 4.5/5。

### 5.4 インプット／アウトプット

| インプット            | アウトプット                       |
| ---------------- | ---------------------------- |
| イベント/アラート, ユーザ報告 | インシデントレコード, ワークアラウンド, PIR 報告 |

### 5.5 ロール

- **サービスデスク**, **オンコールエンジニア**, **インシデントマネージャ**

### 5.6 関連プラクティス

- Problem, Monitoring, Change, Continual Improvement。

### 5.7 失敗しがちなポイント

- 優先度基準が曖昧でリソース分散。

- 再発防止が Problem へエスカレーションされない。

### 5.8 成熟度

| レベル | 特徴                            |
| --- | ----------------------------- |
| Lv3 | オンコールカバレッジ 24×7、SLA 定義。       |
| Lv5 | 自己修復スクリプトで P3 以下を自動解決 60% 以上。 |

### 5.9 ツール例

- PagerDuty, Opsgenie, ServiceNow ITSM。

***

## 6. IT Asset Management

### 6.1 目的・価値

- IT 資産ライフサイクルを最適コストで管理し、リスクとコンプライアンスを統制。

### 6.2 主要活動

1. 資産台帳作成 (HW/SW/クラウド)。

2. ライセンス & 契約管理、監査。

3. 調達・配備・廃棄フロー。

4. CMDB 連携し構成情報を健全化。

### 6.3 成果指標

- ライセンス違反 0 件

- 使用率 80% 以上の資産を 10% 削減

### 6.4 インプット／アウトプット

| インプット          | アウトプット                    |
| -------------- | ------------------------- |
| 資産購買情報, スキャン結果 | 資産台帳, 更新 CMDB, コスト最適化レポート |

### 6.5 ロール

- **IT 資産マネージャ**, **調達担当**, **財務**

### 6.6 関連プラクティス

- Configuration, Financial Mgmt, Security。

### 6.7 失敗しがちなポイント

- シャドー IT を台帳に反映しきれずリスク顕在化。

### 6.8 成熟度

| Lv | 特徴                  |
| -- | ------------------- |
| 2  | 台帳はスプレッドシート。        |
| 4  | Discovery ツールで自動同期。 |

### 6.9 ツール例

- Lansweeper, Flexera, ServiceNow SAM。

***

## 7. Monitoring & Event Management

### 7.1 目的・価値

- サービス状態をリアルタイム可視化し、重大イベントを即時検知・対処。

### 7.2 主要活動

1. Telemetry 収集 (メトリクス, ログ, トレース)。

2. イベント正規化と抑制 (デダブ, 集約)。

3. 相関分析 (AIOps) とルートコーズ推定。

4. 自動化レスポンス (Runbook, Lambda)。

### 7.3 成果指標

- ノイズ削減率 > 90%

- MTTA (検知→対応) < 2 分

### 7.4 インプット／アウトプット

| インプット               | アウトプット                        |
| ------------------- | ----------------------------- |
| Telemetry, CMDB 関係図 | イベントレコード, アラート通知, KPI ダッシュボード |

### 7.5 ロール

- **サイトリライアビリティエンジニア**, **NOC オペレータ**

### 7.6 関連プラクティス

- Incident, Capacity, Availability。

### 7.7 失敗しがちなポイント

- アラート疲れで重要アラート見逃し。

- CMDB 非連携で影響範囲が判定不能。

### 7.8 成熟度

| Lv | 特徴                 |
| -- | ------------------ |
| 3  | 静的閾値アラート。          |
| 5  | ML ベース異常検知 & 自己修復。 |

### 7.9 ツール例

- Datadog, ELK, Dynatrace, Azure Monitor。

***

## 8. Problem Management

### 8.1 目的・価値

- 障害の根本原因を排除し、サービス安定性を継続的に向上。

### 8.2 主要活動

1. インシデントトレンド分析と問題記録。

2. 根本原因分析 (RCA) ― 5 Whys, Ishikawa。

3. 既知エラー (KE) 登録と回避策公開。

4. 恒久対策の実装 (Change 連携)。

### 8.3 成果指標

- 再発インシデント件数 50% 減

- 問題解決までの平均日数 ≤ 14 日

### 8.4 インプット／アウトプット

| インプット        | アウトプット                  |
| ------------ | ----------------------- |
| インシデント履歴, ログ | 問題記録, RCA 報告, KE データベース |

### 8.5 ロール

- **問題マネージャ**, **SME**, **開発チーム**

### 8.6 関連プラクティス

- Incident, Change, Continual Improvement。

### 8.7 失敗しがちなポイント

- RCA が表面的で恒久対策に結びつかない。

- KPI がインシデント削減に連動していない。

### 8.8 成熟度

| Lv | 特徴               |
| -- | ---------------- |
| 2  | 問題登録は手動・任意。      |
| 4  | 自動相関ツールで問題候補を提案。 |

### 8.9 ツール例

- Atlassian JSM, ServiceNow Problem, RCA.io。

***

## 9. Release Management

### 9.1 目的・価値

- ビルド済み変更群を一貫性とガバナンスの下で本番へ展開し、事業価値を迅速提供。

### 9.2 主要活動

1. リリース計画とカレンダー管理。

2. リリースパッケージ作成、バージョン管理。

3. Canary / Blue‑Green / フライトリング展開。

4. バリデーション、ロールバック戦略。

### 9.3 成果指標

- リリース後障害率 < 3%

- 機能デプロイ頻度 ≥ 2/日

### 9.4 インプット／アウトプット

| インプット               | アウトプット                  |
| ------------------- | ----------------------- |
| 承認済み変更, ビルドアーティファクト | リリース計画, デプロイログ, リリースノート |

### 9.5 ロール

- **リリースマネージャ**, **DevOps チーム**, **QA**

### 9.6 関連プラクティス

- Change, Deployment, Validation & Testing。

### 9.7 失敗しがちなポイント

- リリースパッケージ間の依存解決漏れ。

- ロールバック手順未検証。

### 9.8 成熟度

| Lv | 特徴               |
| -- | ---------------- |
| 3  | 週次バッチデプロイ。       |
| 5  | 完全自動デリバリーパイプライン。 |

### 9.9 ツール例

- ArgoCD, Spinnaker, Octopus Deploy。

***

## 10. Service Catalog Management

### 10.1 目的・価値

- ユーザが必要なサービスを一目で把握し、セルフサービスで要求できる環境を提供。

### 10.2 主要活動

1. カタログ階層設計 (ビジネス/技術)。

2. SLA・価格・提供条件を明示。

3. Self‑Service ポータルと要求フロー統合。

4. 継続的なコンテンツレビュー。

### 10.3 成果指標

- カタログ経由要求率 > 80%

- 利用者満足度 ≥ 4.5/5

### 10.4 インプット／アウトプット

| インプット             | アウトプット        |
| ----------------- | ------------- |
| サービスポートフォリオ, 契約情報 | カタログサイト, 更新履歴 |

### 10.5 ロール

- **サービスオーナ**, **カタログマネージャ**

### 10.6 関連プラクティス

- Request Fulfilment, Financial Mgmt。

### 10.7 失敗しがちなポイント

- IT 用語過多でユーザが理解できない。

- 更新漏れ。

### 10.8 成熟度

| Lv | 特徴               |
| -- | ---------------- |
| 2  | 静的 PDF カタログ。     |
| 4  | ポータルで動的見積りと自動承認。 |

### 10.9 ツール例

- ServiceNow Catalog, Freshservice, Azure Marketplace。

***

## 11. Service Configuration Management

### 11.1 目的・価値

- サービスを構成する CI と関係性を可視化し、変更影響分析とリスク最小化を支援。

### 11.2 主要活動

1. CI 識別と命名規約。

2. ディスカバリーツール導入と自動更新。

3. CI 関連マッピングと視覚化。

4. データ品質レビュー (CMDB 健全度 KPI)。

### 11.3 成果指標

- CMDB 完全性 > 95%

- 認可外 CI 0 件

### 11.4 インプット／アウトプット

| インプット                | アウトプット              |
| -------------------- | ------------------- |
| 変更記録, Discovery スキャン | 更新 CMDB, CI リレーション図 |

### 11.5 ロール

- **構成マネージャ**, **CI オーナ**, **ツール管理者**

### 11.6 関連プラクティス

- Change, Asset, Monitoring。

### 11.7 失敗しがちなポイント

- 手動更新依存によるデータ腐敗。

- ビジネスビュー不在で利用価値が低い。

### 11.8 成熟度

| Lv | 特徴                        |
| -- | ------------------------- |
| 3  | Discovery で HW 構成自動取得。    |
| 5  | サービストポロジーと依存度計算をリアルタイム表示。 |

### 11.9 ツール例

- ServiceNow CMDB, Device42, Qualys。

***

## 12. Service Continuity Management

### 12.1 目的・価値

- 重大障害／災害時でもビジネス許容停止時間内にサービスを復旧し、法規制・契約を順守。

### 12.2 主要活動

1. BIA 実施し RTO/RPO 定義。

2. DR アーキテクチャ設計 (Active‑Active, Pilot‑Light)。

3. リカバリープラン文書化 & 定期演習。

4. 事象後レビューと改善。

### 12.3 成果指標

- DR テスト成功率 100%

- 重大インシデント時 RTO 達成率 100%

### 12.4 インプット／アウトプット

| インプット      | アウトプット             |
| ---------- | ------------------ |
| BIA, リスク評価 | BC/DR プラン, テスト結果報告 |

### 12.5 ロール

- **BCM マネージャ**, **DR チーム**, **経営層**

### 12.6 関連プラクティス

- Availability, Risk, Information Security。

### 12.7 失敗しがちなポイント

- 変更後に DR 手順を更新せず陳腐化。

- 演習が机上のみ。

### 12.8 成熟度

| Lv | 特徴                            |
| -- | ----------------------------- |
| 2  | テープバックアップのみ。                  |
| 5  | Cloud DR, IaC でワンクリックフェイルオーバ。 |

### 12.9 ツール例

- AWS DR Runbooks, Zerto, Rubrik。

***

## 13. Service Design

### 13.1 目的・価値

- 新規・変更サービスを "顧客価値 × 実行可能性 × 持続性" の観点で包括的に設計。

### 13.2 主要活動

1. 4 つの次元 (組織・情報・パートナー・バリューストリーム) 分析。

2. UX, 可用性, セキュリティ, オブザーバビリティ設計。

3. サービスブループリント & プロトタイピング。

4. デザインアシュアランスレビュー。

### 13.3 成果指標

- デザイン欠陥によるリワーク率 < 5%

- MVP To‑Market ≤ 8 週間

### 13.4 インプット／アウトプット

| インプット        | アウトプット              |
| ------------ | ------------------- |
| ビジネス要件, 技術標準 | サービスデザインパッケージ (SDP) |

### 13.5 ロール

- **サービスアーキテクト**, **UX デザイナ**, **セキュリティリード**

### 13.6 関連プラクティス

- Business Analysis, Validation & Testing。

### 13.7 失敗しがちなポイント

- 技術視点のみで顧客体験を軽視。

- 過度な設計でリリース遅延。

### 13.8 成熟度

| Lv | 特徴                     |
| -- | ---------------------- |
| 3  | デザインレビューはプロジェクト終盤。     |
| 5  | Design‑Ops、デザインシステム共有。 |

### 13.9 ツール例

- Figma, ArchiMate, AWS Well‑Architected。

***

## 14. Service Desk

### 14.1 目的・価値

- 顧客と IT 組織をつなぐ単一窓口 (SPOC) としてエクスペリエンスを最適化。

### 14.2 主要活動

1. オムニチャネル受付 (電話, Chat, Portal)。

2. ナレッジ記事・スクリプト利用で一次解決率向上。

3. バーチャルエージェントと AI 予測配信。

4. CSAT フィードバックと改善サイクル。

### 14.3 成果指標

- 一次解決率 ≥ 70%

- 平均応答時間 (ASA) ≤ 20 秒

### 14.4 インプット／アウトプット

| インプット         | アウトプット                   |
| ------------- | ------------------------ |
| 顧客問い合わせ, ナレッジ | インシデント/要求チケット, CSAT レポート |

### 14.5 ロール

- **サービスデスクアナリスト**, **バーチャルエージェントトレーナ**

### 14.6 関連プラクティス

- Incident, Request, Knowledge Mgmt。

### 14.7 失敗しがちなポイント

- チケット品質が低く後工程で情報不足。

- CSAT 測定が形骸化。

### 14.8 成熟度

| Lv | 特徴                    |
| -- | --------------------- |
| 2  | 電話中心、手動登録。            |
| 4  | AI Bot により 24×7 自動応答。 |

### 14.9 ツール例

- Zendesk, Freshdesk, MS Power Virtual Agents。

***

## 15. Service Level Management

### 15.1 目的・価値

- ビジネス目標に整合した SLA/OLAs を設定し、パフォーマンスをモニタリング・改善。

### 15.2 主要活動

1. Service Level 要求収集と合意。

2. SLA/OLA/KPI 定義と文書化。

3. レポートと四半期ビジネスレビュー (QBR)。

4. 改善プランとエラーバジェット管理。

### 15.3 成果指標

- SLA 達成率 ≥ 99%

- QBR 改善アクション実施率 ≥ 90%

### 15.4 インプット／アウトプット

| インプット             | アウトプット              |
| ----------------- | ------------------- |
| ビジネス KPI, 可用性レポート | SLA 契約, サービスレベルレポート |

### 15.5 ロール

- **サービスレベルマネージャ**, **アカウントマネージャ**

### 15.6 関連プラクティス

- Continual Improvement, Monitoring。

### 15.7 失敗しがちなポイント

- KPI がビジネス価値と乖離。

- レポートが遅延・手動で信頼性低下。

### 15.8 成熟度

| Lv | 特徴                    |
| -- | --------------------- |
| 3  | Excel レポート。           |
| 5  | リアルタイムダッシュボードと契約自動更新。 |

### 15.9 ツール例

- Power BI, ServiceNow Performance Analytics。

***

## 16. Service Request Management

### 16.1 目的・価値

- 定型サービス要求を効率的・自動で履行し、エンドユーザ体験を向上。

### 16.2 主要活動

1. 標準要求テンプレート化とワークフロー。

2. 自動承認ポリシーとチャットボット履行。

3. 進捗可視化 (ポータル / Slack)。

4. CSAT フィードバック。

### 16.3 成果指標

- 平均履行時間 ≤ 4 時間 (P1 カテゴリ)

- セルフサービス完結率 > 60%

### 16.4 インプット／アウトプット

| インプット        | アウトプット            |
| ------------ | ----------------- |
| カタログ要求, 標準手順 | 完了記録, 自動化 Runbook |

### 16.5 ロール

- **リクエストフルフィルメントチーム**, **RPA 開発者**

### 16.6 関連プラクティス

- Catalog, Asset, Financial。

### 16.7 失敗しがちなポイント

- 可視化不足でユーザが進捗問合せを増発。

### 16.8 成熟度

| Lv | 特徴                    |
| -- | --------------------- |
| 2  | メールベース。               |
| 4  | RPA/Bot で自動履行 50% 以上。 |

### 16.9 ツール例

- UiPath, Freshservice Workflow, Microsoft Power Automate。

***

## 17. Service Validation & Testing

### 17.1 目的・価値

- 新サービス/変更がビジネスおよび技術要件を満たすことを保証し、リリースリスクを低減。

### 17.2 主要活動

1. テスト戦略 & 品質ゲート定義。

2. シフトレフト (ユニット/コンテナテスト)。

3. セキュリティ・パフォーマンステスト統合。

4. UAT / ビジネス検証と承認。

### 17.3 成果指標

- テスト自動化カバレッジ ≥ 85%

- 本番障害のうちテスト検出漏れ率 < 3%

### 17.4 インプット／アウトプット

| インプット          | アウトプット                     |
| -------------- | -------------------------- |
| 要件定義, ビルドパッケージ | テスト報告書, Go/No‑Go 判定, 既知エラー |

### 17.5 ロール

- **QA リード**, **テストエンジニア**, **製品オーナ**

### 17.6 関連プラクティス

- Release, Change, Security。

### 17.7 失敗しがちなポイント

- シナリオカバレッジ不足で本番障害。

- テスト環境が本番と乖離。

### 17.8 成熟度

| Lv | 特徴                        |
| -- | ------------------------- |
| 3  | 手動テスト中心。                  |
| 5  | 全面自動化、契約ベーステストと Chaos 予測。 |

### 17.9 ツール例

- Selenium, Cypress, JMeter, SonarQube。
